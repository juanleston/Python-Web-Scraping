# Python-Web-Scraping
*How to use Scrapy to build your own dataset?*

Wouldn't it be great if we could always find ready to use ready to use datasets anywhere around the web? After being in the Data Analytics field for some time, I have realized that plug and play scenarios are not that common. Sometimes, you just have to roll up your sleeves and build your own.

In that sense, we will use web scraping to gather unstructured data from a website and then organize it into an analytics-ready format. To illustrate how this can be done using Scrapy, I will extract world postal code data from https://worldpostalcode.com/.

## 1. Contents

  - Sitemap.csv:
  - SitemapFormattingVietnam.py:
  - address1.py:
  - address2.py:
  - ScrappyOutputFormatting_Vietnam.py:

## 2. Installation

The installation steps are as follows:

  ### 2.1 Install Anaconda:
    
  ### 2.2 Install Scrapy:

  ### 2.3 Create a new Scrapy Project:

  ### 2.4 Create a list of Start URL's:
  
  ### 2.5 Create Spider:
  
  ### 2.6 Run the Spider:
  
  ### 2.7 Format the output data:

## 3. Usage

## 4. Credits

Resources used to build this repository include:

  - https://towardsdatascience.com/using-scrapy-to-build-your-own-dataset-64ea2d7d4673

## 5. License

![image](https://user-images.githubusercontent.com/60116541/142733137-9ed23afb-0ee8-468e-b0f0-f90f60e70f3c.png)

This work is licensed under a Creative Commons Attribution 3.0 International License
